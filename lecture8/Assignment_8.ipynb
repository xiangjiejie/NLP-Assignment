{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,attention_dropout=0.0):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        self.dropout=nn.Dropout(attention_dropout)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forword(self,q,k,v,scale=None,attn_mask=None):\n",
    "        attention=torch.matmul(q,k,transpose(-2,-1))# 计算 Q*K^T\n",
    "        if scale:\n",
    "            attention=attention*scale\n",
    "        if attn_mask is not None:\n",
    "            attention=attention.masked_fill_(attn_mask,-np.inf)\n",
    "        attention=self.softmax(attention)\n",
    "        attention=self.dropout(attention)\n",
    "        context=torch.matmul(attention,v)\n",
    "        return context\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_modl=512,num_heads=8,dropout=0.0):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.dim_per_head=d_modl\n",
    "        self.num_heads=num_heads\n",
    "        self.linear_k=nn.Linear(d_modl,d_modl)\n",
    "        self.linear_v=nn.Linear(d_modl,d_modl)\n",
    "        self.linear_q=nn.Linear(d_modl,d_modl)\n",
    "        \n",
    "        self.dot_product_attention=ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.linear_final=nn.Linear(d_modl,d_modl)\n",
    "        self.norm=nn.LayerNorm(d_modl)\n",
    "        \n",
    "    def forward(self,keys,values,queries,attn_mask=None):\n",
    "        residual=queries\n",
    "        batch_size=keys.size(0)\n",
    "        keys=self.linear_k(keys)\n",
    "        values=self.linear_v(values)\n",
    "        queries=self.linear_q(queries)\n",
    "        \n",
    "        key=keys.view(batch_size,-1,self.num_heads,self.dim_per_head).transpose(1,2) #重塑张量\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            # unsqueeze 对维度进行扩充 ，repeat维度翻倍\n",
    "            attn_mask-attn_mask.unsqueeze(1).repeat(1,self.num_heads,1,1)\n",
    "            \n",
    "        #**幂运算\n",
    "        scale=(keys.size(-1))**-0.5\n",
    "        \n",
    "        #计算注意力\n",
    "        context=self.dot_product_attention(queries,keys,values,scale,attn_mask)\n",
    "        \n",
    "        #将多个头的输出向量拼接合并\n",
    "        # contiguous \n",
    "        context=context.transpose(1,2).contiguous \\\n",
    "            .view(batch_size,-1,self.num_heads*self.dim_per_head)\n",
    "        \n",
    "        return self.norm(residual+self.linear_final(context)) # linear 将拼接够的多头 进行信息融合和映射回d维度\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "    #前向传播+residual connection\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.0):\n",
    "        super(PositionalWiseFeedForward,self).__init__()\n",
    "        self.w1=nn.Linear(d_model,ffn_dim)\n",
    "        self.w2=nn.Linear(ffn_dim,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output=self.w2(F.relu(self.w1(x)))\n",
    "        # layer normalization and residual network\n",
    "        return self.norm(x+self.dropout(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model=512,num_heads=8,ffn_dim=2018,dropout=0.0):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.attention=MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward=PositionalWiseFeedForward(d_model, ffn_dim, dropout)\n",
    "        \n",
    "    def forward(self,x,attn_mask=None):\n",
    "        context=self.attention(x,x,x,attn_mask)\n",
    "        output=self.feed_forward(context)\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 max_seq_len,\n",
    "                 num_layers = 6,\n",
    "                 d_model = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.encoder_layers=nn.ModuleList([EncoderLayer(d_model,num_heads,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "        self.pos_embedding=PositionalEncoding(d_model, max_seq_len,dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self,x,seq_embedding):\n",
    "        embedding=seq_embedding(x)\n",
    "        output=self.pos_embedding(embedding)\n",
    "        self_attention_mask=padding_mask(x,x)\n",
    "        \n",
    "        for encoder in self.encoder_layers:\n",
    "            output=encoder(output,self_attention_mask)\n",
    "            \n",
    "        return self.norm(output)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(seq_k, seq_q):\n",
    "\n",
    "    # pad sentence\n",
    "    len_q = seq_q.size(1)\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1,len_q,-1)\n",
    "\n",
    "    return pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)\n",
    "    \n",
    "    def forward(self,dec_inputs,enc_outputs,self_attn_mask=None,context_attn_mask=None):\n",
    "        dec_ouput  = self.attention(dec_inputs, dec_inputs, dec_inputs ,self_attn_mask)\n",
    "        dec_ouput = self.attention(enc_outputs, enc_outputs,dec_ouput, context_attn_mask)\n",
    "        dec_output=self.feed_forward(dec_output)\n",
    "        return dec_output\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                 max_seq_len,\n",
    "                 device,\n",
    "                 num_layers = 6,\n",
    "                 d_model  = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.0,\n",
    "                 ):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model,num_heads,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.seq_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, enc_output, seq_embedding, context_attn_mask = None):\n",
    "\n",
    "        embedding = seq_embedding(inputs)\n",
    "        output =  embedding + self.pos_embedding(embedding)\n",
    "\n",
    "        self_attention_padding_mask = padding_mask(inputs, inputs)\n",
    "        seq_mask = sequence_mask(inputs).to(self.device)\n",
    "        self_attn_mask = torch.gt((self_attention_padding_mask+seq_mask), 0 )\n",
    "\n",
    "        for decoder in self.decoder_layers:\n",
    "            output = decoder(output, enc_output,self_attn_mask,context_attn_mask)\n",
    "\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 max_len,\n",
    "                 device,\n",
    "                 num_layers = 6,\n",
    "                 stack_layers= 6,\n",
    "                 d_model = 512,\n",
    "                 num_heads = 8,\n",
    "                 ffn_dim = 2048,\n",
    "                 dropout = 0.2):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder=Encoder(vocab_size,max_len,num_layers,d_model,num_heads,ffn_dim,dropout)\n",
    "        self.decoder=Decoder(vocab_size, max_len,device, num_layers,d_model,num_heads, ffn_dim, dropout)\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "        self.linear=nn.Linear(d_model,vocab_size,bias=False)\n",
    "        \n",
    "    def forward(self,src_seq,dec_tgt,dec_in):\n",
    "        context_attn_mask_dec=padding_mask(dec_tgt,src_tgt)\n",
    "        \n",
    "        en_output=self.encoder(src_seq,embedding)\n",
    "        dec_output=self.decoder(dec_tgt,en_output,self.embedding,context_attn_mask_dec)\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1,2,3,0,0,0],\n",
    "                       [3,4,0,0,0,0],\n",
    "                       [3,0,0,0,0,0],\n",
    "                       [4,5,6,7,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True,  True]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask(inputs,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(seq):\n",
    "\n",
    "    batch_size , seq_len = seq.size()\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len),dtype = torch.uint8),\n",
    "                      diagonal = 1)\n",
    "    mask = mask.unsqueeze(0).expand(batch_size, -1,-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自编码器，人工神经网络的一种，通常用于降维或者特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search在较大数据量的情况下占用很大内存空间，beam search 使用beam size参数来限制在每一步保留下来的可能性词的数量，能找到相对最优解，不一定能找到全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在输入文本中寻找对整句话意思起到重要作用的词语，给每个词分配相应的权重比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量不能解决一词多义的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据已知的上下文来推算下一个词的概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够解决一词多义的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch normalization 适合输入数据分布比较接近，Transformer中每层输入的数据是不一样的，Layer Normalization考虑了输入的所有维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用position embedding处理时序问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention：一句话中的每个词语对于有助于理解整句话含义的权重比例\n",
    "\n",
    "multi-head attention:由多个attention进行计算，不同的头有不同维度的信息，多头操作融合了多种信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masked Multi Self Attention \n",
    "- Layer Norm  \n",
    "- Feed Forward \n",
    "- Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本蕴涵、QA、文本分类等），来通过有标签的数据对模型进行fine-tuning。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机去掉句子中的部分token，然后根据上下文预测被去掉的token值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word Vector\n",
    "- Positional Encoding\n",
    "- Segment Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类任务：对文本最后的向量进行二分类概率计算\n",
    "- 文本标注：利用文本中每个词对应的输出向量对词进行标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT： 使用transformer中去掉中间Encoder-Decoder Attention层的decoder，单向语言模型，pre-training的结构\n",
    "- BERT：使用transformer的encoder，双向语言模型，pre-training + fine-tuning的结构，不能做生成式任务\n",
    "- GPT2：和GPT结构一样，参数更多，训练数据更大"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
